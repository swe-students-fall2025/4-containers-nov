# âœ‹ HandSense â€” Containerized Machine Learning + Web Dashboard System

![ML Client CI](https://github.com/swe-students-fall2025/4-containers-nov/actions/workflows/ml-client-ci.yml/badge.svg)
![Web App CI](https://github.com/swe-students-fall2025/4-containers-nov/actions/workflows/web-app-ci.yml/badge.svg)
![Lint-free](https://github.com/nyu-software-engineering/containerized-app-exercise/actions/workflows/lint.yml/badge.svg)

A fully containerized, three-service application that performs **real-time hand gesture recognition** using a MediaPipe + PyTorch machine-learning client, stores gesture events inside **MongoDB**, and visualizes them through a **Flask-based web dashboard**.

This project demonstrates how separate services communicate inside a Dockerized micro-service architecture.

---

## ğŸ‘¥ Teammates

- [Ivan Wang](https://github.com/Ivan-Wang-tech)  
- [Harrison Gao](https://github.com/HTK-G)  
- [Sina Liu](https://github.com/SinaL0123)  
- [Serena Wang](https://github.com/serena0615)  
- [Hanqi Gui](https://github.com/hanqigui)

---

## ğŸ§± System Overview

The system consists of **three Dockerized services**:

```
+------------------------+     +-----------------------+     +------------------------+
|   Machine Learning     |     |       MongoDB         |     |       Web App          |
|       Client           | --> |   handsense database  | --> |   Dashboard (Flask)    |
| (MediaPipe + PyTorch)  |     |     Gesture_events    |     |   Visualize gestures   |
+------------------------+     +-----------------------+     +------------------------+
```

### ğŸ”¹ Machine-Learning Client  
Runs locally or inside Docker.  
It uses a webcam â†’ detects hands using MediaPipe â†’ predicts gestures using a PyTorch MLP â†’ inserts events into `handsense.gesture_events` collection.

### ğŸ”¹ MongoDB  
Stores gesture logs, statistics, and capture state toggles.

### ğŸ”¹ Web App  
Reads gesture events from MongoDB and presents a dashboard showing:

- Live latest gesture  
- Gesture distribution  
- Recent event timeline  
- Toggle capture control (`/api/control`)

After all services run, you can visit:

ğŸ‘‰ **http://localhost:5000**

---

## ğŸ“ Project Structure

```text
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ instructions.md
â”œâ”€â”€ LICENSE
â”œâ”€â”€ machine-learning-client
â”‚   â”œâ”€â”€ data
â”‚   â”‚   â”œâ”€â”€ hagrid_keypoints_X.npy
â”‚   â”‚   â””â”€â”€ hagrid_keypoints_y.npy
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ models
â”‚   â”‚   â”œâ”€â”€ gesture_mlp.pt
â”‚   â”‚   â””â”€â”€ train_mlp.py
â”‚   â”œâ”€â”€ Pipfile
â”‚   â”œâ”€â”€ Pipfile.lock
â”‚   â”œâ”€â”€ src
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ extract_keypoints_from_hagrid.py
â”‚   â”‚   â””â”€â”€ live_mediapipe_mlp.py
â”‚   â””â”€â”€ tests
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ test_extract_keypoints_from_hagrid.py
â”‚       â””â”€â”€ test_live_mediapipe_mlp.py
â”œâ”€â”€ README.md
â””â”€â”€ web-app
    â”œâ”€â”€ app.py
    â”œâ”€â”€ Dockerfile
    â”œâ”€â”€ Pipfile
    â”œâ”€â”€ Pipfile.lock
    â”œâ”€â”€ readme.txt
    â”œâ”€â”€ static
    â”‚   â”œâ”€â”€ audios
    â”‚   â”‚   â”œâ”€â”€ among_us.mp3
    â”‚   â”‚   â”œâ”€â”€ android_beep.mp3
    â”‚   â”‚   â”œâ”€â”€ bom.mp3
    â”‚   â”‚   â”œâ”€â”€ error.mp3
    â”‚   â”‚   â”œâ”€â”€ playme.mp3
    â”‚   â”‚   â”œâ”€â”€ rick_roll.mp3
    â”‚   â”‚   â”œâ”€â”€ rizz.mp3
    â”‚   â”‚   â”œâ”€â”€ sponge_bob.mp3
    â”‚   â”‚   â””â”€â”€ uwu.mp3
    â”‚   â”œâ”€â”€ hagrid_classes.json
    â”‚   â”œâ”€â”€ images
    â”‚   â”‚   â”œâ”€â”€ fist.png
    â”‚   â”‚   â”œâ”€â”€ like.png
    â”‚   â”‚   â”œâ”€â”€ ok.png
    â”‚   â”‚   â”œâ”€â”€ one.png
    â”‚   â”‚   â”œâ”€â”€ palm.png
    â”‚   â”‚   â”œâ”€â”€ stop.png
    â”‚   â”‚   â”œâ”€â”€ thinking.png
    â”‚   â”‚   â”œâ”€â”€ three.png
    â”‚   â”‚   â””â”€â”€ two_up.png
    â”‚   â”œâ”€â”€ script.js
    â”‚   â””â”€â”€ style.css
    â”œâ”€â”€ templates
    â”‚   â””â”€â”€ index.html
    â””â”€â”€ tests
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ conftest.py
        â””â”€â”€ test_app.py
```

---

## âš™ï¸ 1. Environment Setup (Any Platform)

The recommended workflow uses **pipenv** for dependency management.

### macOS / Linux / Windows (WSL)

#### Install pipenv
```bash
pip install pipenv
```

---

## âš™ï¸ 2. Running the System (Docker)

From project root:

```bash
docker compose up --build
```

This starts:

| Service | URL | Purpose |
|---------|-----|---------|
| web-app | http://localhost:5000 | Dashboard UI |
| mongodb | localhost:27017 | Database |
| ml-client | headless, no UI | Captures gestures + inserts into DB |

To stop:

```bash
docker compose down
```

---

## ğŸ‘ï¸ Running the ML Client With Webcam (macOS/Windows/Linux Host)

Since macOS Docker cannot access `/dev/video0`, we run the ML client on host machine:

```bash
cd machine-learning-client
pipenv install --dev
pipenv run python src/live_mediapipe_mlp.py
```

Features:

- Live webcam feed
- MediaPipe hand-tracking
- PyTorch gesture inference
- Inserts gesture records into `handsense.gesture_events`
- Press `q` to quit

---

## ğŸ—„ï¸ 3. MongoDB Configuration + Starter Data

The database name is:

```
handsense
```

Collections automatically created:

| Collection | Purpose |
|------------|---------|
| gesture_events | ML client inserts gesture data |
| controls | Stores capture toggle state |

At first run the ML client ensures:

```json
{
  "_id": "capture_control",
  "enabled": false
}
```

---

## ğŸ” 4. Environment Variables

Both ml-client and web-app use these:

| Variable | Description |
|----------|-------------|
| MONGO_URI | Mongo connection string (default: `mongodb://mongodb:27017`) |
| MONGO_DB_NAME | Database name (default: `handsense`) |
| SECRET_KEY | Flask sessions |

See `.env.example` below.

---

## ğŸ“„ 5. .env.example (Required for TA Submission)

Place this file in project root:

```env
# MongoDB configuration
MONGO_URI=mongodb://mongodb:27017
MONGO_DB_NAME=handsense

# Flask secret
SECRET_KEY=dev-secret
```

Then create an actual `.env`:

```bash
cp .env.example .env
```

---

## ğŸ” 6. Web App (Flask) â€” Running Locally

```bash
cd web-app
pipenv install --dev
pipenv run flask run --host=0.0.0.0 --port=5000
```

Navigate to:

ğŸ‘‰ **http://localhost:5000**

### Endpoints:

| Route | Description |
|-------|-------------|
| `/` | Dashboard UI |
| `/api/latest` | Latest gesture |
| `/api/latest_full` | Latest gesture (detailed) |
| `/api/control` | POST toggle capture |
| `/api/control/status` | GET capture control |

---

## ğŸ§ª 7. Testing + Linting + Coverage

### Run ML Client Tests
```bash
cd machine-learning-client
pipenv run pytest --cov=src
pipenv run pylint src
```

### Run Web App Tests
```bash
cd web-app
pipenv run pytest --cov=.
pipenv run pylint app.py
```

Coverage must be â‰¥ 80%.

---

## ğŸ§° 8. Docker Compose

```yaml
version: "3.9"

services:
  mongodb:
    image: mongo:6
    container_name: mongodb
    ports:
      - "27017:27017"
    volumes:
      - mongo-data:/data/db

  web-app:
    build:
      context: ./web-app
    container_name: web-app
    depends_on:
      - mongodb
    environment:
      MONGO_URI: "mongodb://mongodb:27017"
      MONGO_DB_NAME: "handsense"
      FLASK_APP: "app.py"
      FLASK_RUN_HOST: "0.0.0.0"
    ports:
      - "5000:5000"

  ml-client:
    build:
      context: ./machine-learning-client
    container_name: ml-client
    depends_on:
      - mongodb
    environment:
      MONGO_URI: "mongodb://mongodb:27017"
      MONGO_DB_NAME: "handsense"

volumes:
  mongo-data:
```
